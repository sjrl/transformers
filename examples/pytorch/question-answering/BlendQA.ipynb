{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e589d9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seb/miniconda3/envs/sjrl_transformers/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from prep_data import get_blendqa, _prep_mrqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c22aeeef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mrqa (/home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)\n",
      "100%|██████████| 3/3 [00:00<00:00, 180.25it/s]\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-fc04cfb8f9e2c4cb.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-dc5f31fa00d0b5e2.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-6ec73bdddd3313b6.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-ea5d7acc42cfa6a9.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-b5e83be035e9a112.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-e3c986cbba93147a.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-0270080a4ed7e603.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-52c4f9b426484a9b.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-31c636e5c6ba2d09.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-32b983cef6b7c9f9.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-d88492166e891e62.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-a7ec7115a6cc8ddb.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-72eaae597a34b10f.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-374140f3ec81a5c5.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-35c63b2c3399be7d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 58 training rows\n",
      "Removed 5 validation rows\n",
      "Removed 82 test rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad_v2 (/home/seb/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n",
      "100%|██████████| 2/2 [00:00<00:00, 31.24it/s]\n",
      "Found cached dataset adversarial_qa (/home/seb/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n",
      "100%|██████████| 3/3 [00:00<00:00, 134.61it/s]\n",
      "Found cached dataset syn_qa (/home/seb/.cache/huggingface/datasets/mbartolo___syn_qa/synQA/1.0.0/0ebec640592fc3d0a99f119a04b6e2e75a205b54ed951b6c54d9808ee40f1790)\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n",
      "                                                                                 \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['subset', 'context', 'id', 'question', 'answers'],\n",
       "        num_rows: 905303\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['subset', 'context', 'id', 'question', 'answers'],\n",
       "        num_rows: 62582\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['subset', 'context', 'id', 'question', 'answers'],\n",
       "        num_rows: 12551\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blendqa(\n",
    "    preprocessing_num_workers=1,\n",
    "    overwrite_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491f0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mrqa (/home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)\n",
      "100%|██████████| 3/3 [00:00<00:00, 138.20it/s]\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-fc04cfb8f9e2c4cb.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-dc5f31fa00d0b5e2.arrow\n",
      "Loading cached processed dataset at /home/seb/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19/cache-6ec73bdddd3313b6.arrow\n",
      "Formatting MRQA Answers:  19%|█▉        | 82820/430231 [00:07<00:56, 6109.73 examples/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 81914 Subset: TriviaQA-web Answer: '30 mph' vs. '30mph '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  20%|██        | 86603/430231 [00:07<01:01, 5591.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 85802 Subset: TriviaQA-web Answer: '110m' vs. '110 '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  23%|██▎       | 97971/430231 [00:09<01:00, 5477.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 97309 Subset: TriviaQA-web Answer: 'euler s identity' vs. 'Euler's identity'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  23%|██▎       | 99907/430231 [00:10<00:59, 5528.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 99460 Subset: TriviaQA-web Answer: 'gj 581g' vs. 'GJ 581 '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  26%|██▌       | 112367/430231 [00:12<00:59, 5342.03 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 111271 Subset: TriviaQA-web Answer: 'boys brigade' vs. 'Boys’ Brigad'\n",
      "Idx: 112220 Subset: TriviaQA-web Answer: 'miners track' vs. 'Miners’ Trac'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  29%|██▉       | 126575/430231 [00:15<00:56, 5396.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 126135 Subset: TriviaQA-web Answer: 'british airline pilots association' vs. 'British Airline Pilots’ Associatio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  32%|███▏      | 136245/430231 [00:16<00:51, 5732.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 135216 Subset: TriviaQA-web Answer: 'dunkin donuts' vs. 'Dunkin’ Donut'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  32%|███▏      | 138957/430231 [00:17<00:37, 7776.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 137247 Subset: SearchQA Answer: 'Pee Wee’s Big Adventure' vs. 'Pee-wee's Big Adventure'\n",
      "Idx: 137290 Subset: SearchQA Answer: '\"Porky’s Duck Hunt\"' vs. 'Porky's Duck Hunt, '\n",
      "Idx: 137293 Subset: SearchQA Answer: '\"Someday We’ll Be Together\"' vs. 'Someday, We'll Be Together '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  33%|███▎      | 141489/430231 [00:17<00:35, 8165.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 140537 Subset: SearchQA Answer: '\"Lady Chatterley\\'s Lover\"' vs. 'Lady Chatterley s Lover is'\n",
      "Idx: 140730 Subset: SearchQA Answer: 'nay/neigh' vs. 'nay, neig'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  34%|███▎      | 144479/430231 [00:17<00:33, 8426.23 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 142985 Subset: SearchQA Answer: 'Deer/venison' vs. 'deer) * veni'\n",
      "Idx: 143408 Subset: SearchQA Answer: 'pain/pane' vs. 'pain, pan'\n",
      "Idx: 143999 Subset: SearchQA Answer: 'Burma/Myanmar' vs. 'Burma (Myanma'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting MRQA Answers:  34%|███▍      | 145770/430231 [00:18<00:33, 8490.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 145436 Subset: SearchQA Answer: 'that’s the way the cookie crumbles' vs. 'Thats the way the cookie crumbles '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  35%|███▍      | 149609/430231 [00:18<00:33, 8284.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 148325 Subset: SearchQA Answer: 'N (month/moth)' vs. 'N (month/\n",
      "moth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  35%|███▌      | 152164/430231 [00:18<00:33, 8377.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 150477 Subset: SearchQA Answer: 'Yeti/Abominable Snowman' vs. 'Yeti: Abominable Snowma'\n",
      "Idx: 151740 Subset: SearchQA Answer: 'jaguar/leopard' vs. 'Jaguar & Leopa'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  36%|███▌      | 155226/430231 [00:19<00:31, 8595.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 153789 Subset: SearchQA Answer: 'Hawaii/Sandwich Islands' vs. 'HAWAII (SANDWICH ISLAND'\n",
      "Idx: 154163 Subset: SearchQA Answer: 'Export/Import Bank' vs. 'Export-Import Bank'\n",
      "Idx: 154284 Subset: SearchQA Answer: 'Crown/cap' vs. 'crown (ca'\n",
      "Idx: 154738 Subset: SearchQA Answer: 'Cassava/manioc' vs. 'cassava (manio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  37%|███▋      | 159000/430231 [00:19<00:32, 8418.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 157765 Subset: SearchQA Answer: 'Russet/Burbank' vs. 'Russet Burbank'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  38%|███▊      | 163610/430231 [00:20<00:31, 8462.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 162254 Subset: SearchQA Answer: 'Edward VIII/Duke of Windsor' vs. 'EDWARD VIII (DUKE OF WINDSO'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  39%|███▊      | 166188/430231 [00:20<00:31, 8484.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 164983 Subset: SearchQA Answer: 'arc/ark' vs. 'arc, ar'\n",
      "Idx: 165247 Subset: SearchQA Answer: 'Parthenon/Temple of Athena' vs. 'PARTHENON / TEMPLE OF \n",
      "ATH'\n",
      "Idx: 165761 Subset: SearchQA Answer: 'Reek/wreak' vs. 'Reek, Wrea'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  40%|███▉      | 171374/430231 [00:21<00:29, 8671.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 169854 Subset: SearchQA Answer: 'brays/braise' vs. 'brays/\n",
      "brais'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  41%|████      | 174784/430231 [00:21<00:30, 8274.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 173227 Subset: SearchQA Answer: 'foam/insulation' vs. 'Foam Insulation'\n",
      "Idx: 174359 Subset: SearchQA Answer: 'Married... with Children' vs. 'Married...With Children '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  42%|████▏     | 178796/430231 [00:21<00:29, 8646.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 178328 Subset: SearchQA Answer: 'Mr. Freeze (Dr. Victor Fries)' vs. 'Mr. Freeze/Dr. Victor Fries .'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  45%|████▍     | 191923/430231 [00:23<00:27, 8810.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 190384 Subset: SearchQA Answer: 'refrigerator & freezer' vs. 'Refrigerator/Freezer .'\n",
      "Idx: 191673 Subset: SearchQA Answer: 'The Netherlands/Holland' vs. 'Netherlands, Holland or'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  45%|████▌     | 194930/430231 [00:23<00:26, 8755.04 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 193302 Subset: SearchQA Answer: 'Nissan/Datsun' vs. 'Nissan. Datsu'\n",
      "Idx: 194009 Subset: SearchQA Answer: 'Venus/Aphrodite' vs. 'Venus, (Aphrodi'\n",
      "Idx: 194766 Subset: SearchQA Answer: 'ping-pong/table tennis' vs. 'Ping Pong Table Tennis'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  46%|████▌     | 198377/430231 [00:24<00:26, 8687.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 197264 Subset: SearchQA Answer: 'Fireflies/lightning bugs' vs. 'fireflies & lightning bu'\n",
      "Idx: 197868 Subset: SearchQA Answer: 'Hush....Hush, Sweet Charlotte' vs. 'Hush Hush Sweet Charlotte (19'\n",
      "Idx: 198484 Subset: SearchQA Answer: 'Bull Run/Manassas' vs. 'Bull Run (Manassa'\n",
      "Idx: 198784 Subset: SearchQA Answer: 'Maxwell Smart/Agent 86' vs. 'Maxwell Smart, Agent 8'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  47%|████▋     | 200311/430231 [00:24<00:27, 8252.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 199256 Subset: SearchQA Answer: 'Cheops/Khufu' vs. 'CHEOPS \n",
      "(KHU'\n",
      "Idx: 199498 Subset: SearchQA Answer: 'Falkland Islands/Islas Malvinas' vs. 'Falkland Islands (Islas Malvina'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  47%|████▋     | 203368/430231 [00:24<00:26, 8556.97 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 202921 Subset: SearchQA Answer: 'Uranus & Neptune' vs. 'Uranus/Neptune  '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  48%|████▊     | 207211/430231 [00:25<00:26, 8494.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 206657 Subset: SearchQA Answer: 'alcohol/liquor' vs. 'alcohol/\n",
      "liquo'\n",
      "Idx: 206680 Subset: SearchQA Answer: 'shopping/grocery cart' vs. 'Shopping Grocery Cart'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  49%|████▉     | 212752/430231 [00:25<00:26, 8296.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 211346 Subset: SearchQA Answer: 'North (thorn)' vs. 'north/thorn, '\n",
      "Idx: 212746 Subset: SearchQA Answer: 'Bryan Adams, Rod Stewart & Sting' vs. 'Bryan Adams/Rod Stewart/\n",
      "Sting, '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  50%|████▉     | 214920/430231 [00:26<00:25, 8512.49 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 213742 Subset: SearchQA Answer: 'Married....with Children' vs. 'Married... with \n",
      "Childre'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  52%|█████▏    | 223636/430231 [00:27<00:23, 8668.47 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 222774 Subset: SearchQA Answer: 'New York & Paris' vs. 'New York/Paris t'\n",
      "Idx: 222991 Subset: SearchQA Answer: 'St. Patrick\\'s Day' vs. 'St.Patrick's Day e'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  53%|█████▎    | 228785/430231 [00:27<00:23, 8524.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 227796 Subset: SearchQA Answer: 'I-D' vs. 'id,'\n",
      "Idx: 229114 Subset: SearchQA Answer: 'Zebra & Donkey' vs. 'zebra/donkey z'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  54%|█████▍    | 233523/430231 [00:28<00:22, 8609.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 232028 Subset: SearchQA Answer: 'John Lennon & Paul McCartney' vs. 'John Lennon/Paul McCartney c'\n",
      "Idx: 232293 Subset: SearchQA Answer: 'One Flew Over the Cuckoo\\'s Nest' vs. 'One Flew Over The Cuckoo S Nest '\n",
      "Idx: 233171 Subset: SearchQA Answer: 'Hollywood & Vine' vs. 'Hollywood/Vine i'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting MRQA Answers:  55%|█████▍    | 234851/430231 [00:28<00:22, 8687.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 234227 Subset: SearchQA Answer: 'The Devil\\'s Advocate' vs. 'Devil S Advocate - Pa'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  56%|█████▋    | 242529/430231 [00:29<00:22, 8517.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 241168 Subset: SearchQA Answer: 'Magellan/Magellanic Clouds' vs. 'Magellan, Magellanic Cloud'\n",
      "Idx: 242463 Subset: SearchQA Answer: 'Uncle Tom’s Cabin' vs. 'Uncle Tom's Cabin'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  58%|█████▊    | 247789/430231 [00:30<00:20, 8731.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 246831 Subset: SearchQA Answer: 'taught/taut' vs. 'Taught? tau'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  58%|█████▊    | 250928/430231 [00:30<00:20, 8834.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 250253 Subset: SearchQA Answer: 'Little Rascals/Our Gang' vs. 'Little Rascals & Our Ga'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  23%|██▎       | 2241/9633 [00:00<00:00, 11282.47 examples/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 731 Subset: BioASQ Answer: 'The ubiquitin/proteasome pathway' vs. 'ubiquitin-proteasome pathway as '\n",
      "Idx: 732 Subset: BioASQ Answer: 'The ubiquitin/proteasome pathway' vs. 'ubiquitin-proteasome pathway pla'\n",
      "Idx: 733 Subset: BioASQ Answer: 'The ubiquitin/proteasome pathway' vs. 'ubiquitin-proteasome pathway has'\n",
      "Idx: 734 Subset: BioASQ Answer: 'The ubiquitin/proteasome pathway' vs. 'ubiquitin-proteasome pathway reg'\n",
      "Idx: 735 Subset: BioASQ Answer: 'The ubiquitin/proteasome pathway' vs. 'ubiquitin-proteasome pathway is '\n",
      "Idx: 1053 Subset: BioASQ Answer: 'BCR-ABL' vs. 'BCR/ABL'\n",
      "Idx: 1100 Subset: BioASQ Answer: 'BCR-ABL' vs. 'BCR/ABL'\n",
      "Idx: 1105 Subset: BioASQ Answer: 'BCR-ABL' vs. 'BCR/ABL'\n",
      "Idx: 1261 Subset: BioASQ Answer: 'CRISPR-Cas' vs. 'CRISPR/Cas'\n",
      "Idx: 1440 Subset: BioASQ Answer: 'The BCR/ABL gene fusion' vs. 'BCR-ABL gene fusion in '\n",
      "Idx: 1443 Subset: BioASQ Answer: 'ABL/BCR fusion' vs. 'ABL-BCR fusion'\n",
      "Idx: 1444 Subset: BioASQ Answer: 'The BCR/ABL gene fusion' vs. 'BCR-ABL gene fusion was'\n",
      "Idx: 1445 Subset: BioASQ Answer: 'The BCR/ABL gene fusion' vs. 'BCR-ABL gene fusion is '\n",
      "Idx: 1446 Subset: BioASQ Answer: 'ABL/BCR fusion' vs. 'ABL-BCR fusion'\n",
      "Idx: 3011 Subset: DuoRC.ParaphraseRC Answer: 'Jill.' vs. 'll (M'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Formatting MRQA Answers:  37%|███▋      | 3605/9633 [00:00<00:00, 10056.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 3042 Subset: DuoRC.ParaphraseRC Answer: 'the firefox' vs. 'refox\" by N'\n",
      "Idx: 3045 Subset: DuoRC.ParaphraseRC Answer: 'Bilyarsk,' vs. 'lyarsk ou'\n",
      "Idx: 3058 Subset: DuoRC.ParaphraseRC Answer: 'The Wilson gang.' vs. 'lson gang, while'\n",
      "Idx: 3075 Subset: DuoRC.ParaphraseRC Answer: 'A plaque.' vs. 'aque is g'\n",
      "Idx: 3077 Subset: DuoRC.ParaphraseRC Answer: 'Nano Cells' vs. 'no-cells i'\n",
      "Idx: 3078 Subset: DuoRC.ParaphraseRC Answer: 'the society' vs. 'ciety quick'\n",
      "Idx: 3079 Subset: DuoRC.ParaphraseRC Answer: 'Nano Cells' vs. 'no-cells i'\n",
      "Idx: 3091 Subset: DuoRC.ParaphraseRC Answer: 'A Gun.' vs. 'n goes'\n",
      "Idx: 3091 Subset: DuoRC.ParaphraseRC Answer: 'A gun' vs. 'n goe'\n",
      "Idx: 3097 Subset: DuoRC.ParaphraseRC Answer: 'a groupie' vs. 'oupie (Je'\n",
      "Idx: 3101 Subset: DuoRC.ParaphraseRC Answer: 'small pink rag doll that rarely moves' vs. 'all, pink rag doll that rarely moves.'\n",
      "Idx: 3102 Subset: DuoRC.ParaphraseRC Answer: 'Attack the vampire nest' vs. 'tack a vampire nest, Pa'\n",
      "Idx: 3104 Subset: DuoRC.ParaphraseRC Answer: 'Stacey.' vs. 'acey (K'\n",
      "Idx: 3113 Subset: DuoRC.ParaphraseRC Answer: 'Norris.' vs. 'rris wh'\n",
      "Idx: 3189 Subset: DuoRC.ParaphraseRC Answer: 'A diner' vs. 'ner in '\n",
      "Idx: 3227 Subset: DuoRC.ParaphraseRC Answer: 'The Organization' vs. 'ganization, and '\n",
      "Idx: 3227 Subset: DuoRC.ParaphraseRC Answer: '\"The Organization\"' vs. 'ganization, and to'\n",
      "Idx: 3293 Subset: DuoRC.ParaphraseRC Answer: 'dragon-training' vs. 'agon training. '\n",
      "Idx: 3319 Subset: DuoRC.ParaphraseRC Answer: 'Lt.Tony Carlson.' vs. '. Tony Carlson. '\n",
      "Idx: 3335 Subset: DuoRC.ParaphraseRC Answer: 'Jake.' vs. 'ke, t'\n",
      "Idx: 3338 Subset: DuoRC.ParaphraseRC Answer: 'Devices that fool organ scanners used by repo men' vs. 'vices that fool organ scanners used by repo-men. '\n",
      "Idx: 3357 Subset: DuoRC.ParaphraseRC Answer: 'Ellis Island.' vs. 'lis Island an'\n",
      "Idx: 3375 Subset: DuoRC.ParaphraseRC Answer: 'Teddy-2.' vs. 'ddy-2 at'\n",
      "Idx: 3429 Subset: DuoRC.ParaphraseRC Answer: 'Into oven' vs. 'to the ov'\n",
      "Idx: 3433 Subset: DuoRC.ParaphraseRC Answer: 'Memmet.' vs. 'mmet wa'\n",
      "Idx: 3476 Subset: DuoRC.ParaphraseRC Answer: 'Honey Ryder.' vs. 'ney Ryder, d'\n",
      "Idx: 3512 Subset: DuoRC.ParaphraseRC Answer: 'A bullet' vs. 'llet was'\n",
      "Idx: 3515 Subset: DuoRC.ParaphraseRC Answer: 'left-hand' vs. 'ft hand. '\n",
      "Idx: 3548 Subset: DuoRC.ParaphraseRC Answer: 'on a Tuesday' vs. ' Tuesday, ju'\n",
      "Idx: 3555 Subset: DuoRC.ParaphraseRC Answer: 'In 1919.' vs. ' 1919 En'\n",
      "Idx: 3576 Subset: DuoRC.ParaphraseRC Answer: 'Her mother.' vs. 'r mother, a'\n",
      "Idx: 3611 Subset: DuoRC.ParaphraseRC Answer: 'Spider Pig' vs. 'ider-Pig. '\n",
      "Idx: 3619 Subset: DuoRC.ParaphraseRC Answer: 'an eviction notice' vs. 'iction notice and '\n",
      "Idx: 3638 Subset: DuoRC.ParaphraseRC Answer: 'Behind the fireplace' vs. 'hind a fireplace, bu'\n",
      "Idx: 3640 Subset: DuoRC.ParaphraseRC Answer: 'Venrees.' vs. 'nrees [G'\n",
      "Idx: 3641 Subset: DuoRC.ParaphraseRC Answer: 'The Bat.' vs. 't has ta'\n",
      "Idx: 3654 Subset: DuoRC.ParaphraseRC Answer: 'Eddie Mars.' vs. 'die Mars, s'\n",
      "Idx: 3656 Subset: DuoRC.ParaphraseRC Answer: 'General Sternwood.' vs. 'neral Sternwood ha'\n",
      "Idx: 3687 Subset: DuoRC.ParaphraseRC Answer: 'Mr. Neville' vs. ' Neville is'\n",
      "Idx: 3688 Subset: DuoRC.ParaphraseRC Answer: 'Mr. Noyes' vs. ' Noyes (N'\n",
      "Idx: 3690 Subset: DuoRC.ParaphraseRC Answer: 'MR. HERBERT'S' vs. ' Herbert's in'\n",
      "Idx: 3691 Subset: DuoRC.ParaphraseRC Answer: 'Mrs. Herbert' vs. 's Herbert in'\n",
      "Idx: 3708 Subset: DuoRC.ParaphraseRC Answer: 'Durand Durand' vs. 'rand-Durand. '\n",
      "Idx: 3718 Subset: DuoRC.ParaphraseRC Answer: 'In the Australian outback.' vs. ' the Australian outback in'\n",
      "Idx: 3771 Subset: DuoRC.ParaphraseRC Answer: 'The jewish refugees' vs. 'wish refugees in th'\n",
      "Idx: 3772 Subset: DuoRC.ParaphraseRC Answer: 'Into the woods?' vs. 'to the woods. T'\n",
      "Idx: 3775 Subset: DuoRC.ParaphraseRC Answer: 'A campsite' vs. 'mpsite, th'\n",
      "Idx: 3831 Subset: DuoRC.ParaphraseRC Answer: 'A death star' vs. 'ath Star.Nea'\n",
      "Idx: 3855 Subset: DuoRC.ParaphraseRC Answer: 'Their water hole has dried up.' vs. 'eir water hole has dried up an'\n",
      "Idx: 3857 Subset: DuoRC.ParaphraseRC Answer: 'Space wizard?' vs. 'ace wizard hi'\n",
      "Idx: 3860 Subset: DuoRC.ParaphraseRC Answer: 'Fan boy?' vs. 'n-boy ju'\n",
      "Idx: 3861 Subset: DuoRC.ParaphraseRC Answer: 'basement?' vs. 'sement, w'\n",
      "Idx: 3901 Subset: DuoRC.ParaphraseRC Answer: 'a virus' vs. 'rus mut'\n",
      "Idx: 3902 Subset: DuoRC.ParaphraseRC Answer: 'a Parrotmon' vs. 'rrotmon, a '\n",
      "Idx: 3907 Subset: DuoRC.ParaphraseRC Answer: 'The first Digimon' vs. 'rst Digimon. Kari'\n",
      "Idx: 3919 Subset: DuoRC.ParaphraseRC Answer: 'Seven days.' vs. 'ven days on'\n",
      "Idx: 3940 Subset: DuoRC.ParaphraseRC Answer: 'John Elsinore.' vs. 'hn Elsinore ha'\n",
      "Idx: 3944 Subset: DuoRC.ParaphraseRC Answer: 'An orphanage' vs. 'phanage, whi'\n",
      "Idx: 3954 Subset: DuoRC.ParaphraseRC Answer: 'Half sister' vs. 'lf-sister G'\n",
      "Idx: 3959 Subset: DuoRC.ParaphraseRC Answer: 'the Manager' vs. 'nager) ente'\n",
      "Idx: 4000 Subset: DuoRC.ParaphraseRC Answer: 'a car' vs. 'r up '\n",
      "Idx: 4024 Subset: DuoRC.ParaphraseRC Answer: 'Elise's butler, Leopoldo Mastelloni' vs. 'ise's butler (Leopoldo Mastelloni) '\n",
      "Idx: 4064 Subset: DuoRC.ParaphraseRC Answer: 'not-guilty' vs. 't guilty. '\n",
      "Idx: 4091 Subset: DuoRC.ParaphraseRC Answer: 'a knife' vs. 'ife of '\n",
      "Idx: 4106 Subset: DuoRC.ParaphraseRC Answer: 'An unemployed entertainer.' vs. 'employed entertainer Faust'\n",
      "Idx: 4108 Subset: DuoRC.ParaphraseRC Answer: 'A dancer.' vs. 'ncer Magg'\n",
      "Idx: 4161 Subset: DuoRC.ParaphraseRC Answer: 'The Leprechaun.' vs. 'prechaun and ma'\n",
      "Idx: 4162 Subset: DuoRC.ParaphraseRC Answer: 'The Leprechaun.' vs. 'prechaun and ma'\n",
      "Idx: 4163 Subset: DuoRC.ParaphraseRC Answer: 'The Leprechaun.' vs. 'prechaun and ma'\n",
      "Idx: 4164 Subset: DuoRC.ParaphraseRC Answer: 'The Leprechaun.' vs. 'prechaun and ma'\n",
      "Idx: 4199 Subset: DuoRC.ParaphraseRC Answer: 'a flower pot' vs. 'ower pot is '\n",
      "Idx: 4218 Subset: DuoRC.ParaphraseRC Answer: 'a troll' vs. 'oll]], '\n",
      "Idx: 4219 Subset: DuoRC.ParaphraseRC Answer: 'Hrothgar, king of Danelands' vs. 'othgar]], king of Danelands'\n",
      "Idx: 4248 Subset: DuoRC.ParaphraseRC Answer: 'a sycamore tree' vs. 'camore tree tha'\n",
      "Idx: 4276 Subset: DuoRC.ParaphraseRC Answer: 'His camera.' vs. 's camera an'\n",
      "Idx: 4278 Subset: DuoRC.ParaphraseRC Answer: 'A dead body.' vs. 'ad body (Jan'\n",
      "Idx: 4297 Subset: DuoRC.ParaphraseRC Answer: 'A year' vs. 'ar, sh'\n",
      "Idx: 4297 Subset: DuoRC.ParaphraseRC Answer: 'a year' vs. 'ar, sh'\n",
      "Idx: 4320 Subset: DuoRC.ParaphraseRC Answer: '\"Van Gogh\"' vs. 'n Gogh!\" S'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  62%|██████▏   | 6016/9633 [00:00<00:00, 8616.46 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 4381 Subset: DuoRC.ParaphraseRC Answer: 'His wife, Kelly' vs. 's wife Kelly (R'\n",
      "Idx: 4427 Subset: DuoRC.ParaphraseRC Answer: 'The car' vs. 'r when '\n",
      "Idx: 4453 Subset: DuoRC.ParaphraseRC Answer: 'Carol.' vs. 'rol on'\n",
      "Idx: 4454 Subset: DuoRC.ParaphraseRC Answer: 'Lady Beldon.' vs. 'dy Beldon (D'\n",
      "Idx: 4458 Subset: DuoRC.ParaphraseRC Answer: 'A wounded German pilot.' vs. 'unded German pilot (Hel'\n",
      "Idx: 4481 Subset: DuoRC.ParaphraseRC Answer: 'Train.' vs. 'ain fo'\n",
      "Idx: 4485 Subset: DuoRC.ParaphraseRC Answer: 'Dr Loomis' vs. '. Loomis '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 8857 Subset: TextbookQA Answer: 'force = work/distance.' vs. 'Force = Work Distance '\n",
      "Idx: 8858 Subset: TextbookQA Answer: 'distance = work/force.' vs. 'Distance = Work Force '\n",
      "Idx: 8947 Subset: TextbookQA Answer: 'speed = distance/time.' vs. 'Speed = Distance Time '\n",
      "Idx: 9462 Subset: TextbookQA Answer: 'time = distance/speed.' vs. 'time = distance speed '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  22%|██▏       | 10506/47714 [00:01<00:06, 5764.44 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 9669 Subset: TriviaQA-web Answer: 'doc' vs. '   '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  28%|██▊       | 13479/47714 [00:01<00:05, 6637.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 12555 Subset: SearchQA Answer: 'input & output' vs. 'input/output s'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  37%|███▋      | 17777/47714 [00:02<00:03, 8257.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 16992 Subset: SearchQA Answer: 'the A\\'s' vs. ''s      '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  53%|█████▎    | 25201/47714 [00:03<00:02, 8184.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 23869 Subset: SearchQA Answer: 'Look Who\\'s Talking' vs. 'Look Who s Talking '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting MRQA Answers:  57%|█████▋    | 27339/47714 [00:03<00:02, 8357.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 25828 Subset: SearchQA Answer: 'we/wee' vs. 'we/\n",
      "we'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 58 training rows\n",
      "Removed 5 validation rows\n",
      "Removed 82 test rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    }
   ],
   "source": [
    "mrqa_datasets = _prep_mrqa(\n",
    "    preprocessing_num_workers=1,\n",
    "    overwrite_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82df088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['subset', 'context', 'id', 'question', 'answers'],\n",
       "        num_rows: 430173\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['subset', 'context', 'id', 'question', 'answers'],\n",
       "        num_rows: 9551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['subset', 'context', 'id', 'question', 'answers'],\n",
       "        num_rows: 47709\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrqa_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0c50df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subset': 'TriviaQA-web',\n",
       " 'context': '            Sports | Greg Lemond Trades His Bike For A Race Car ...Sports | Greg Lemond Trades His Bike For A Race Car -- Retired Cyclist Says He Will Switch From 2 Wheels To 4 | Seattle Times Newspaper       Greg Lemond Trades His Bike For A Race Car -- Retired Cyclist Says He Will Switch From 2 Wheels To 4       AP       Greg LeMond has fallen in love with four wheels.       The retired three-time world cycling champion and Tour de France winner was looking for something to give him the same kind of charge he used to get from the two-wheel sport. He found auto racing.       LeMond will drive in the USAC-SCCA Formula Ford 2000 Pro Series for Miller Brothers Racing in 1997.       \"I missed the action,\" he said. \"I\\'m a racer. I don\\'t care for the training in bike racing. I don\\'t miss that. But you don\\'t need to do that kind of training to race a car.\"       LeMond, 35, will take over the ride left vacant by FF2000 series champion Steve Knapp, who will stay with the Miller team but move up to either Indy Lights or Toyota-Atlantic, both stepping stones to the PPG Indy Car World Series.       Could the same path be in LeMond\\'s future?       \"It\\'s way too soon to be thinking about anything like that,\" he said. \"I\\'d like to do anything in car racing and I\\'d love to be driving Indy cars, but that\\'s a far-off dream.\"       Knapp will help LeMond learn the ropes, especially chassis set-ups and aerodynamics.       \"I\\'ll probably be in the back of the pack for a while,\" LeMond said. \"I don\\'t know where my potential is. I just know I\\'m pretty hooked on this sport.\"       \"Don\\'t look for me to be where Steve is right now. Give me time. I\\'ve only had about 16 days in the car.\"       John Miller, the team owner and a driving teammate, approached LeMond after watching him finish 12th earlier this year in a rain-soaked pro racing debut in the SCCA Spec Racer Ford Pro Series at Minneapolis. The team later tested him at Blackhawk Farms Raceway at Rockford, Ill., in early September.       \"This is not just a celebrity ride,\" Miller said. \"We think there is a serious potential here. Greg is old for bike racing, but not for auto racing.\"       LeMond retired from cycling in 1994 because he was diagnosed with a rare muscular disease called mitochondria myopathy, which affected his ability to compete in the sport.       \"I was diagnosed with a genetic disease that only affects me at the top end of my performance,\" he said. \"It shouldn\\'t have any effect on me in a race car.\"       LeMond said his cycling education should stand him in good stead on four wheels.       \"In the Tour de France, you\\'re doing 40-50 miles per hour in a group of about 60 guys going downhill,\" he said. \"I think the similarities are learning the course and knowing it well and knowing your tactics.       \"In racing, cycles or cars, you always have to be looking ahead, and your reaction time is very similar. Both sports are very intense and you have to stay very focused.\"       LeMond is also very aware of the physical danger that is always present in his new sport.       \"My wife is worrying about the safety aspects of racing, but there\\'s danger in any sport. A lot of people die in bike racing,\" he said. \"Sixty miles per hour coming down a mountain pass without a helmet is pretty dangerous, too.       \"You don\\'t think about that part of it in either sport. You just prepare the best you can, get the best equipment you can get and go out and do the best you can.\"       Copyright (c) 1996 Seattle Times Company, All Rights Reserved.            Greg LeMond becoming a Tour de France institutionGreg LeMond becoming a Tour',\n",
       " 'id': 'c6acbec44ebd448c854d31984d262d2b',\n",
       " 'question': 'Greg LeMond was a champion in which type of sport?',\n",
       " 'answers': {'text': ['cycling', 'Cyclist'], 'answer_start': [2517, 263]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrqa_datasets[\"train\"][100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec6f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import(\n",
    "  AutoModelForQuestionAnswering,\n",
    "  AutoTokenizer,\n",
    "  pipeline\n",
    ")\n",
    "model_name = \"sjrhuschlee/flan-t5-base-squad2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e4b2180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)okenizer_config.json: 2.35kB [00:00, 4.53MB/s]\n",
      "\n",
      "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.67MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]\u001b[A\n",
      "Downloading (…)/main/tokenizer.json: 2.42MB [00:00, 22.4MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 22.9kB/s]\n",
      "\n",
      "Downloading (…)cial_tokens_map.json: 2.23kB [00:00, 4.40MB/s]\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba59ce6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁[', 'DOC', ']', '▁[', 'T', 'LE', ']', '▁Sports', '▁test']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"[DOC] [TLE] Sports    test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30c6cb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Sports', '▁test']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"            Sports    test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45881802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Sports    test      \n",
      "[DOC] [TLE] Sports    test [DOC]\n"
     ]
    }
   ],
   "source": [
    "tmp = \"[DOC] [TLE] Sports    test [DOC]\".replace(\"[DOC]\", \" \"*5)\n",
    "tmp = tmp.replace(\"[TLE]\", \" \"*5)\n",
    "print(tmp)\n",
    "print(\"[DOC] [TLE] Sports    test [DOC]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb77553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
